{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b1a563-e1a3-47a2-8e5d-ad0e680ae416",
   "metadata": {},
   "source": [
    "# Merge parallel scans\n",
    "Merge scans collected in parallel with a common *master.h5* file, as implemented in `hula`.  \n",
    "Parallel scans are typically measured using a multi-sample holder mounted on the xy-stage, such as the *AMPIX* or battery setup.  \n",
    "  \n",
    "The note book reads metadata and scan indices from the *master.h5* file, followed by the azimuthally integrated data from the parallel *\\*pilatus_integrated.h5* files, and writes all to a *master_pilatus_integrated.h5* file.\n",
    "  \n",
    "**Parameters:**  \n",
    "`raw_path` : Path to the *raw* files. Can be modified to specify a subdirectory  \n",
    "`embed_meta` : Toggle whether to embed metadata from the *master.h5* into the *master_pilatus_integrated.h5* file  \n",
    "`only_new` : Toggle whether to only merge new files where a master_pilatus_integrated.h5 file does not exist  \n",
    "`delete_redundant_files` : Toggle whether to delete the *_pilatus_integrated.h5* files after merging to the *master_pilatus_integrated.h5* file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02814f-0801-4592-a1dd-4850ed05a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "from _parallel import parallelMetaMaster, getParallelAzintData, reintegrateParallelFiles\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc29a53-bf31-4ae1-bbc0-dcfc43f85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_path = '/data/visitors/danmax/proposal/visit/raw/'\n",
    "raw_path = os.getcwd().replace('/scripts','/raw') + # + '/[subfolder]'\n",
    "embed_meta = True              # embed meta data in the master_pilatus_integrated.h5 file\n",
    "only_new   = True              # only merge files where a master_pilatus_integrated.h5 file does not exist\n",
    "delete_redundant_files = True  # delete the redundant _pilatus_integrated.h5 files after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee3348-7d33-4aa0-b803-e1b9ff279614",
   "metadata": {},
   "source": [
    "#### loop through *master.h5* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f772a-54da-4214-a401-2da6d40ff3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all master.h5 files in the raw_path\n",
    "files = sorted(glob.glob(f'{raw_path}**/master.h5', recursive=True), key = os.path.getctime, reverse=True)\n",
    "if only_new:\n",
    "    # remove file names from the list if a **master_pilatus_integrated.h5 file already exists\n",
    "    files = [f for f in files if not os.path.isfile(f.replace('raw','process/azint').replace('master.h5','master_pilatus_integrated.h5'))]\n",
    "    \n",
    "for i,fname in enumerate(files):\n",
    "    print(f'{i+1} of {len(files)} - {\"raw\"+fname.split(\"/raw\")[-1]:<150s}')#,end='\\r') \n",
    "    # read scan ids and metadata from the master.h5 file\n",
    "    try:\n",
    "        print('   Reading data...',end='\\r')\n",
    "        scan_ids, metadata, start_pos = parallelMetaMaster(fname)\n",
    "    except OSError:\n",
    "        # if and OSError is raised, skip to the next file\n",
    "        print(f'Skipping {fname}')\n",
    "        continue\n",
    "    # make a list of all relevant _pilatus_integrated.h5 files\n",
    "    azi_path = os.path.dirname(fname.replace('raw','process/azint')\n",
    "    scan_list = [azi_path + f'/{scan}_pilatus_integrated.h5' for scan in scan_ids]\n",
    "    # read the integrated data\n",
    "    azidata = getParallelAzintData(scan_list)\n",
    "\n",
    "    # write to output file\n",
    "    print('   Writing to file...',end='\\r')\n",
    "    azi_master_path = azi_path + f'/master_pilatus_integrated.h5'\n",
    "    with h5py.File(azi_master_path,'w') as f:\n",
    "        for key in azidata:\n",
    "            f.create_dataset(key, data=azidata[key])\n",
    "        if embed_meta:\n",
    "            for key in metadata:\n",
    "                f.create_dataset(f'meta/{key}', data=metadata[key])\n",
    "            for key in start_pos:\n",
    "                f.create_dataset(f'meta/start_positioners/{key}', data=start_pos[key])\n",
    "\n",
    "    if delete_redundant_files:\n",
    "        print('   Deleting files... ',end='\\r')\n",
    "        for scan in scan_list:\n",
    "            try:\n",
    "                os.remove(scan)\n",
    "            except PermissionError:\n",
    "                print('    Unable to delete files')\n",
    "                break\n",
    "                \n",
    "print(f'{\"Done!\":25s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9510f15-2d77-4622-8491-6368c8a43d23",
   "metadata": {},
   "source": [
    "## Re-integrate *master.h5* files\n",
    "Re-integrate all entries in a master file from a list of master files using the same integration configurations.  \n",
    "The re-integrated *master_pilatus_integrated.h5* files are saved under *[proposal]/[visit]/process/azint/reintegrated*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe169b0-761a-48f5-95c9-4e889882519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define integration configuration parameters\n",
    "config = {'poni_file'          : '/data/visitors/danmax/proposal/visit/process/[poni_file].poni',\n",
    "          'mask'               : '/data/visitors/danmax/proposal/visit/process/[mask_file].npy',\n",
    "          'radial_bins'        : 3000,\n",
    "          'azimuth_bins'       : None,\n",
    "          'unit'               : '2th',         # '2th' or 'q'\n",
    "          'n_splitting'        : 15,\n",
    "          'polarization_factor': 0.999997}\n",
    "\n",
    "# toggle embedded meta data\n",
    "embed_meta = True\n",
    "\n",
    "files = sorted(glob.glob(f'{raw_path}/**/master.h5', recursive=True), key = os.path.getctime, reverse=True)\n",
    "print('List of master.h5 files:')\n",
    "for f in files:\n",
    "    print(f.split('/raw')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c830fc4-039b-4108-ae22-3f902ce4a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are you sure you wish to re-integrate? [y/n]')\n",
    "if input()=='y':\n",
    "    reintegrateParallelFiles(files, config, embed_meta=embed_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441613c-67de-40f8-a7a5-0a1a79c82694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDF5 / Simple Analysis / GPU",
   "language": "python",
   "name": "maxiv-jhub-docker-kernel-hdf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
