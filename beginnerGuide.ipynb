{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af660db-2bbe-4bd5-be7a-6c09044cccf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Beginners guide to DanMAX data handling and analysis\n",
    "\n",
    "This notebook is intended as a beginners guide to working with the DanMAX data. The notebook will showcase some simple examples of how to work with the DanMAX data in jupyter notebook. You can use this notebook as inspiration to help ypu process your data and quickly prepare plots  \n",
    "\n",
    "### Data structure\n",
    "The data at DanMAX are saved in the HDF5 format (*.h5*), a hierachical data format. The files are generally divided into three catagories: master, raw, and processed. The master file contains the meta data (time stamp, motor positions, energy, etc.), the raw file contains the raw detector data, and the processed file contains the integrated data. Furthermore, the master file has an external link to the raw file, which makes it easier to access the data. All scans (measurements) have a scan number used to identify the relevant files. It is important to keep track of the scan numbers to know which files are important data and which are irrelevant alignment scans. The files are generally named *scan-####.h5*, with *####* being the scan number. The raw files have the detector name as suffix, e.g. *scan-####_pilatus.h5* and the processed files have the detector name + the data reduction method as suffix, e.g. *scan-####_pilatus_integrated.h5*.  \n",
    "  \n",
    "### DanMAX.py\n",
    "The `DanMAX.py` file contains a wide selection of useful python functions for working with data at DanMAX. The functions are easily imported with `import DanMAX as DM` and called with `DM.`. If you wish to copy the scripts and notebooks to a local folder, then make sure to include the `__.init__.py` file as well. Use `help(DM)` to get a list of all the DanMAX functions.  \n",
    "\n",
    "### Content:\n",
    "\n",
    "[**Read datafrom the .h5 files**](#read_data)  \n",
    "[**Read datafrom the .h5 files (manual)**](#read_data_man)  \n",
    "[**Ploting a heatmap**](#plot_heat)  \n",
    "[**Saving data in other formats**](#saving)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60176a79-c2da-4c2c-9d22-9594638319f9",
   "metadata": {},
   "source": [
    "#### Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4628ed-b8eb-4109-b771-01642f897f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DanMAX.py Version 2.0.0\n",
      "Current proposal and visit:\n",
      "/data/visitors/danmax/20230160/2023101208/scripts_\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import DanMAX as DM\n",
    "print('Current proposal and visit:')\n",
    "print(os.getcwd().split('scripts/')[0][:-7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e5db6-95d7-4fff-984c-c7c7f77617b7",
   "metadata": {},
   "source": [
    "#### Read data from the .h5 files <a id='read_data'></a>  \n",
    "The `DanMAX.py` library already comes with functions for reading the data from the .h5 files, namley `DM.getAzintData()` and ` DM.getMetaData()`. Both functions return a python *dictionary* containing the data names (keys) and values.   \n",
    "\n",
    "The most common meta data is easily read with `DM.getMetaData(fname)`. For an extended meta data dictionary, use `DM.getMetaDic(fname)`  \n",
    "Use `DM.getMetaDic(fname).keys()` to get a list of the available meta data  \n",
    "Use `help(DM.getAzintData)`, `help(DM.getMetaData)` and `help(DM.getMetaDic)` for more information about reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701c558-f1e6-4ab6-af80-9b1afd48e799",
   "metadata": {},
   "source": [
    "First we specify the full path and file name of the master file. This can either be done manually as a string or with the `DM.findScan()` function. If left empty, `DM.findScan()` will return the latest completed scan in the proposal or it can be provided with a scan number to find a specific scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68c6d8-c268-43c3-b03c-2680b956f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert path for the .h5 file - TIP: Use tap for auto-complete\n",
    "#fname = '/data/visitors/danmax/PROPOSAL/VISIT/raw/SAMPLE/scan-XXXX.h5'\n",
    "fname = DM.findScan() # automatically find the latest scan in the proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52252624-4eb4-4323-8c0f-1206752c70fb",
   "metadata": {},
   "source": [
    "The file name of the integrated data can be found from the master file name or be manually provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25171e53-3650-49a0-b015-d20a25cfca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the azimuthally integrated filename from master file name\n",
    "aname = DM.getAzintFname(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999859b-de53-4850-a293-66a52b1216f5",
   "metadata": {},
   "source": [
    "The data are easily read with the DM functions and we can then assign some more usefull variable names.  \n",
    "  \n",
    "Because of the 10-minute ring current top-up, it is a good idea to normalize the diffraction data with the incident beam intensity $I_0$, as this will remove (most of) the intensity variation.  \n",
    "We do the normalization in the same cell as the data import, to avoid accidentally normalizing the same data several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc10767-a0bd-4b53-af83-26971dc4383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the integrated data\n",
    "data = DM.getAzintData(aname)\n",
    "# read common meta data from the master file\n",
    "meta = DM.getMetaData(fname)\n",
    "\n",
    "# determine if the diffraction data use Q or 2theta\n",
    "if type(data['q']) != type(None):\n",
    "    x = data['q']\n",
    "    Q = True\n",
    "else:\n",
    "    x = data['tth']\n",
    "    Q = False\n",
    "\n",
    "# assign new variable names\n",
    "I = data['I']\n",
    "t = meta['time'] # relative time stamp in seconds\n",
    "T = meta['temp'] # temperature in Kelvin (if available, otherwise None)\n",
    "I0 = meta['I0']  # relative incident beam intensity \"I zero\"\n",
    "E = meta['energy'] # X-ray energy in keV\n",
    "\n",
    "## normalize the diffraction intensities with the incident beam intensity I0\n",
    "# the data are first transposed, then normalized, and then transposed back again\n",
    "I = (I.T/I0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870675ee-8c99-45cc-bc52-883c208225cf",
   "metadata": {},
   "source": [
    "##### Plot average diffraction pattern\n",
    "We can use `numpy` to quickly calculate the average diffraction pattern and plot it using `matplotlib.pyplot`.  \n",
    "We need to specify which axis we wish to take the mean along. The diffraction data `I` has the shape \\[*frames*, *radial bins*], so to get the (time) average pattern, we specify the *\"frame\"*-axis *axis=0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcbb8c-d1cc-4dc0-89b8-887dc2b47f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average diffraction pattern\n",
    "I_avg = np.mean(I,axis=0)\n",
    "\n",
    "## plot the average pattern as function of x ##\n",
    "# initialize the figure\n",
    "plt.figure()\n",
    "# plot the data\n",
    "plt.plot(x, I_avg, label='Average pattern')\n",
    "# set axes labels to Q or 2theta\n",
    "if Q:\n",
    "    plt.xlabel('Q')\n",
    "else:\n",
    "    plt.xlabel('2theta')\n",
    "plt.ylabel('Intensity')\n",
    "# add legend with the label specified in plt.plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b6ff5-1048-4c99-ba87-7747315f5675",
   "metadata": {},
   "source": [
    "#### Read data from the .h5 files (manual) <a id='read_data_man'></a>  \n",
    "While the `DM.getAzintData()` and ` DM.getMetaData()` functions are very convenient for most applications, sometimes one might wish to manually read directly from the *.h5* files.  \n",
    "We will reuse the azimuthally integrated file name (*aname*) from the previous cells, but this time we will read the data with the `h5py` module.  \n",
    "\n",
    "It is good practice to use *context managers* when reading/writing files in python. This ensures that the file is only open when we need it and closed automatically when we are done.  \n",
    "\n",
    "One can think of the *.h5* files as virtual folders and subfolders. To get the data, we first need to navigate to the right folder (called *group*) and the read the data (called *dataset*). The azimuthally integrated diffraction data are located at:  \n",
    "*entry/data1d/I*  \n",
    "To read the data as a numpy array, we add `[:]` at the end. If the data are not an array but instead a scalar, we add `[()]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e462a52-d663-498c-951f-d9dd04def694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the file context manager in 'read' mode\n",
    "with h5.File(aname,'r') as f:\n",
    "    I = f['entry/data1d/I'][:]\n",
    "\n",
    "# as soon as we end the indentation, the file is closed by the context manager\n",
    "print(I.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8ce56-f889-440d-8449-a0dc23ee23d6",
   "metadata": {},
   "source": [
    "The manual approach comes in handy when the datasets are *very* large and we start to run out of computer memory. In that case we can read the data bit by bit, so we don't need to store everything in the memory.  \n",
    "As an example we will calculate the average diffraction image for a dataset, without reading all images in the raw file at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7223124-95b5-4d25-9938-5be1e88516d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the file context manager in 'read' mode\n",
    "with h5.File(fname,'r') as f:\n",
    "    # get the total number of frames in the file\n",
    "    no_of_frames = f['/entry/instrument/pilatus/data'].shape[0]\n",
    "    print(f'{no_of_frames} frames in scan')\n",
    "    # read the first frame at index zero\n",
    "    im = f['/entry/instrument/pilatus/data'][0]\n",
    "    \n",
    "    # iterate through all remaining frames and add the values to the initial frame\n",
    "    for i in range(1,no_of_frames):\n",
    "        im += f['/entry/instrument/pilatus/data'][i]\n",
    "\n",
    "# divide the sum of all frames with the number of frames\n",
    "im_avg = im/no_of_frames\n",
    "\n",
    "# plot the average image\n",
    "plt.figure()\n",
    "plt.imshow(im_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fa07f-7f97-482c-9963-39d0f3653205",
   "metadata": {},
   "source": [
    "#### Plotting a heatmap <a id='plot_heat'></a>  \n",
    "A very common way to visualize time-resolved diffraction data is with a heatmap (not to be confused with a waterfall plot)  \n",
    "Make sure to include the incident beam intensity $I_0$ normalization during the data import, to remove the systematic intensity variation caused by the ring top-up  \n",
    "  \n",
    "We generally recommend to use `pcolormesh` for heatmaps, as it can handle non equidistant data, which is very convenient when converting between $2\\theta$ and $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60661830-3172-4c22-bb30-4dee5a899ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig = plt.figure()\n",
    "# set figure title\n",
    "plt.title('Heatmap')\n",
    "# add ticks to the right axis\n",
    "plt.tick_params('y',right=True)\n",
    "# set axis labels\n",
    "if Q:\n",
    "    plt.xlabel(r'Q [$\\AA^{-1}$]')\n",
    "else:\n",
    "    plt.xlabel(r'2$\\theta$ [$\\deg$]')\n",
    "plt.ylabel('Time (s)')\n",
    "\n",
    "# create plot\n",
    "plt.pcolormesh(x,          # radial data (theta/Q)\n",
    "               t,          # time\n",
    "               I,          # diffraction data\n",
    "               norm='log', # normalization option (here log scale)\n",
    "              )\n",
    "\n",
    "# add a colorbar\n",
    "plt.colorbar(label='log(I)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f8d0f-01d9-49f5-8a97-ac10c7a5e9f7",
   "metadata": {},
   "source": [
    "#### Saving data in other formats (*.xy .xye .dat*) <a id='saving'></a>  \n",
    "We **highly** recommend working with the *.h5* format as much as possible, however, we realize that many analysis software require column-separated file formats. This type of file tends to take up a lot of storage space and slow down file browser systems.  \n",
    "It is therefore <b style=\"color:red;\">NOT ALLOWED</b> to export to this format at MAX IV!  \n",
    "Instead, perfom the data export on your local system and only for the relevant data. Likewise, if you plan to sum the data to reduce the time-resolution, do this *before* exporting to column-separated files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08aa70-d2cb-4d64-9e0d-8933e4fee9a5",
   "metadata": {},
   "source": [
    "A script for exporting the data to column-separated *.xy* files could look like this:\n",
    "```\n",
    "import os\n",
    "import numpy as np\n",
    "import DanMAX as DM\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# file name of master file\n",
    "fname = 'myFolder/scan-0001.h5'\n",
    "# file destination\n",
    "destination = 'myFolder/xy_files'\n",
    "\n",
    "# reduce time-resolution to improve statistics\n",
    "rf = 1        # reduction factor\n",
    "start = None  # first frame index (if None use default)\n",
    "end =  None   # last  frame index (if None use default)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "aname = DM.getAzintFname(fname)\n",
    "# read the integrated data\n",
    "data = DM.getAzintData(aname)\n",
    "if type(data['q']) != type(None):\n",
    "    Q = True\n",
    "else:\n",
    "    Q = False\n",
    "# read common meta data from the master file\n",
    "meta = DM.getMetaData(fname)\n",
    "\n",
    "# apply data reduction\n",
    "data = DM.reduceDic(data,reduction_factor=rf,start=start,end=end)\n",
    "meta = DM.reduceDic(meta,reduction_factor=rf,start=start,end=end)\n",
    "\n",
    "# assign new variable names\n",
    "I = data['I']\n",
    "I0 = meta['I0']  # relative incident beam intensity \"I zero\"\n",
    "E = meta['energy'] # X-ray energy in keV\n",
    "E = np.mean(E)\n",
    "## normalize the diffraction intensities with the incident beam intensity I0\n",
    "# the data are first transposed, then normalized, and then transposed back again\n",
    "I = (I.T/I0).T\n",
    "\n",
    "I *=rf # multiply by the reduction factor to retain absolute counts\n",
    "\n",
    "# calculate effective time-resolution\n",
    "dt = np.mean(np.diff(t[scan]))\n",
    "print(f'Effective time-resolution: {dt:.2f} s')\n",
    "\n",
    "# make file header\n",
    "header=['DanMAX diffraction data',\n",
    "       f'Energy (keV): {E:.2f}',\n",
    "       f'Wavelength (A): {DM.keV2A(E):.4f}',\n",
    "       f'Effective time-resolution (s): {dt:.3f}',\n",
    "       ]\n",
    "if rf > 1:\n",
    "    # add information about the data reduction factor\n",
    "    header += [f'Data reduction factor: {reduction_factor}',\n",
    "               f'Frames: {i*reduction_factor} to {reduction_factor*(i+1)-1}',\n",
    "              ]\n",
    "if Q:\n",
    "    header += ['Q(A-1)      I(counts)']\n",
    "else:\n",
    "    header += ['tth(deg)    I(counts)']\n",
    "\n",
    "# iterate through the integrated data\n",
    "for i,y in enumerate(I):\n",
    "    # set destination file name\n",
    "    dst = os.path.join(destination,f'{DM.getScan_id(fname)}_{i:05d}.xy')\n",
    "    # stack the x- and y-data in columns\n",
    "    columns = np.stack([x,y]).T\n",
    "    # save to file\n",
    "    np.savetxt(dst,                            # file destination path\n",
    "               columns,                        # column data\n",
    "               delimiter=' ',                  # column delimiter\n",
    "               comments = '#',                 # header prefix\n",
    "               fmt= ['%6.4f','%10.2f'],        # formatting (allocated space and decimal points)\n",
    "               header='\\n'.join(header))       # file header\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDF5 / Simple Analysis / GPU",
   "language": "python",
   "name": "maxiv-jhub-docker-kernel-hdf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
