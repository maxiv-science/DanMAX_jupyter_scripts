{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b1a563-e1a3-47a2-8e5d-ad0e680ae416",
   "metadata": {},
   "source": [
    "# Merge parallel scans\n",
    "Merge scans collected in parallel with a common *master.h5* file, as implemented in `hula`.  \n",
    "Parallel scans are typically measured using a multi-sample holder mounted on the xy-stage, such as the *AMPIX* or battery setup.  \n",
    "  \n",
    "The note book reads metadata and scan indices from the *master.h5* file, followed by the azimuthally integrated data from the parallel *\\*pilatus_integrated.h5* files, and writes all to a *master_pilatus_integrated.h5* file.\n",
    "  \n",
    "**Parameters:**  \n",
    "`raw_path` : Path to the *raw* files. Can be modified to specify a subdirectory  \n",
    "`embed_meta` : Toggle whether to embed metadata from the *master.h5* into the *master_pilatus_integrated.h5* file  \n",
    "`only_new` : Toggle whether to only merge new files where a master_pilatus_integrated.h5 file does not exist  \n",
    "`delete_redundant_files` : Toggle whether to delete the *_pilatus_integrated.h5* files after merging to the *master_pilatus_integrated.h5* file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02814f-0801-4592-a1dd-4850ed05a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py as h5\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import DanMAX as DM\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc29a53-bf31-4ae1-bbc0-dcfc43f85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_path = '/data/visitors/danmax/proposal/visit/raw/'\n",
    "raw_path = os.getcwd().replace('/scripts/batteries','/raw')  # + '/[subfolder]'\n",
    "#embed_meta = False              # embed meta data in the master_pilatus_integrated.h5 file\n",
    "only_new   = False              # only merge files where a master_pilatus_integrated.h5 file does not exist\n",
    "delete_redundant_files = False  # delete the redundant _pilatus_integrated.h5 files after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee3348-7d33-4aa0-b803-e1b9ff279614",
   "metadata": {},
   "source": [
    "#### loop through *master.h5* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f772a-54da-4214-a401-2da6d40ff3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all master.h5 files in the raw_path\n",
    "files = sorted(glob.glob(f'{raw_path}/**/master.h5', recursive=True), key = os.path.getctime, reverse=True)\n",
    "if only_new:\n",
    "    # remove file names from the list if a **master_pilatus_integrated.h5 file already exists\n",
    "    files = [f for f in files if not os.path.isfile(f.replace('raw','process/azint').replace('master.h5','master_pilatus_integrated.h5'))]\n",
    "    \n",
    "for i,fname in enumerate(files):\n",
    "    print(f'{i+1} of {len(files)} - {\"raw\"+fname.split(\"/raw\")[-1]:<150s}')#,end='\\r') \n",
    "    # read scan ids and metadata from the master.h5 file\n",
    "    try:\n",
    "        print('   Reading data...',end='\\r')\n",
    "        scan_ids, metadata, start_pos = DM.parallel.parallelMetaMaster(fname)\n",
    "    except [OSError, KeyError]:\n",
    "        # if and OSError is raised, skip to the next file\n",
    "        print(f'Skipping {fname}')\n",
    "        continue\n",
    "    # make a list of all relevant _pilatus_integrated.h5 files\n",
    "    azi_path = os.path.dirname(fname.replace('raw','process/azint'))\n",
    "    scan_list = [azi_path + f'/{scan}_pilatus_integrated.h5' for scan in scan_ids]\n",
    "    scan_list = [scan for scan in scan_list if os.path.isfile(scan)]\n",
    "    # read the integrated data\n",
    "    azidata, azimeta = DM.parallel.getParallelAzintData(scan_list)\n",
    "\n",
    "    # write to output file\n",
    "    print('   Writing to file...',end='\\r')\n",
    "    azi_master_path = azi_path + f'/master_pilatus_integrated.h5'\n",
    "\n",
    "    DM.integration.writeAzintFile(azi_master_path,azidata,azimeta)\n",
    "    \n",
    "    # # excluded keys\n",
    "    # excl_keys = ['date', 'input', 'program', 'version']\n",
    "    # with h5.File(azi_master_path,'w') as f:\n",
    "    #     for key in azidata:\n",
    "    #         if not azidata[key] is None and not key in excl_keys:\n",
    "    #             f.create_dataset(key, data=azidata[key])\n",
    "    #     if embed_meta:\n",
    "    #         for key in metadata:\n",
    "    #             if not metadata[key] is None:\n",
    "    #                 f.create_dataset(f'meta/{key}', data=metadata[key])\n",
    "    #         for key in start_pos:\n",
    "    #             if not start_pos[key] is None:\n",
    "    #                 f.create_dataset(f'meta/start_positioners/{key}', data=start_pos[key])\n",
    "\n",
    "    if delete_redundant_files:\n",
    "        print('   Deleting files... ',end='\\r')\n",
    "        for scan in scan_list:\n",
    "            try:\n",
    "                os.remove(scan)\n",
    "            except PermissionError:\n",
    "                print('    Unable to delete files')\n",
    "                break\n",
    "                \n",
    "print(f'{\"Done!\":25s}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDF5 / Simple Analysis / GPU",
   "language": "python",
   "name": "maxiv-jhub-docker-kernel-hdf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
